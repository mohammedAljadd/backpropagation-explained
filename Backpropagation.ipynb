{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Explaining Backpropagation easily</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To better understand backpropagation let's take the following example :\n",
    "\n",
    "<img src='https://miro.medium.com/max/1086/1*dkpb3XSLslX9IjIAGrSYsA.png' width='350'>\n",
    "\n",
    "and let y = $sigmoid(w_{1}.x_{1}+w_{2}.x_{2}+w_{3}.x_{3}+b)$. \n",
    "\n",
    "- This simple form of the neural network is simply a logistic regression classifier. \n",
    "\n",
    "- Let $L(y^{k},y_{true}^{k})$ the loss function for the k$^{th}$ training example and $J(w,b) = \\frac{1}{m}\\sum_{i=1}^{m} L(y^{i},y_{true}^{i})$ the cost function defined as the sum of all the losses corresponding to m training examples.\n",
    "\n",
    "\n",
    "\n",
    "- Now our goal is to get w and b that minimize the cost function $J$. And we will do this in two main steps :\n",
    "\n",
    "   - 1) Forward propagation is how neural networks make prediction :\n",
    "       - calculate $z = w_{1}.x_{1}+w_{2}.x_{2}+w_{3}.x_{3}+b$.\n",
    "       - calculate y = $sigmoid(z)$\n",
    "       - calculate $L(y,y_{true})$ or $J(w,b)$ for many examples. \n",
    "    \n",
    "   - 2) Backward propagation is an algorithm for supervised learning of artificial neural networks using gradient descent. Given an artificial neural network and an error function, the method calculates the gradient of the error function with respect to the neural network's weights $w's$ and biases $b's$. And we will use this gradient $\\nabla(J) = \\begin{bmatrix}\n",
    "           \\frac{\\partial{J}}{\\partial{w_{1}}} \\\\\n",
    "           \\frac{\\partial{J}}{\\partial{w_{2}}} \\\\\n",
    "           \\frac{\\partial{J}}{\\partial{w_{3}}} \\\\\n",
    "           \\frac{\\partial{J}}{\\partial{b}} \n",
    "         \\end{bmatrix}$, to perform gradient descent :\n",
    "         \n",
    "        - Repeat until convergence {\n",
    "\n",
    "            - $w_{1} = w_{1} - \\alpha.\\frac{\\partial{J}}{\\partial{w_{1}}}$\n",
    "            - $w_{2} = w_{2} - \\alpha.\\frac{\\partial{J}}{\\partial{w_{2}}}$\n",
    "            - $w_{3} = w_{3} - \\alpha.\\frac{\\partial{J}}{\\partial{w_{3}}}$\n",
    "            - $b = b - \\alpha.\\frac{\\partial{J}}{\\partial{b}}$\n",
    "\n",
    "            }\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Generalization</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take an example just for the purpose of illustation :\n",
    " \n",
    "<img src='https://miro.medium.com/max/2636/1*Gh5PS4R_A5drl5ebd_gNrg@2x.png' width='450'>\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
