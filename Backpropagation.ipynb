{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Explaining Backpropagation easily</h1>\n",
    "\n",
    "To better understand backpropagation let's take the following example :\n",
    "\n",
    "<img src='https://miro.medium.com/max/1086/1*dkpb3XSLslX9IjIAGrSYsA.png' width='300'>\n",
    "\n",
    "and let y = $sigmoid(w_{1}.x_{1}+w_{2}.x_{2}+w_{3}.x_{3}+b)$. \n",
    "\n",
    "- This simple form of the neural network is simply a logistic regression classifier. \n",
    "\n",
    "- Let $L(y^{k},y_{true}^{k})$ the loss function for the k$^{th}$ training example and $J(w,b) = \\frac{1}{m}\\sum_{i=1}^{m} L(y^{i},y_{true}^{i})$ the cost function defined as the sum of all the losses corresponding to m training examples.\n",
    "\n",
    "\n",
    "\n",
    "- Now our goal is to get w and b that minimize the cost function $J$. And we will do this in two main steps :\n",
    "\n",
    "   - 1) Forward propagation is how neural networks make prediction :\n",
    "       - calculate $z = w_{1}.x_{1}+w_{2}.x_{2}+w_{3}.x_{3}+b$.\n",
    "       - calculate y = $sigmoid(z)$\n",
    "       - calculate $L(y,y_{true})$ or $J(w,b)$ for many examples. \n",
    "    \n",
    "   - 2) Backward propagation is an algorithm for supervised learning of artificial neural networks using gradient descent. Given an artificial neural network and an error function, the method calculates the gradient of the error function with respect to the neural network's weights $w's$ and biases $b's$. And we will use this gradient $\\nabla(J) = \\begin{bmatrix}\n",
    "           \\frac{\\partial{J}}{\\partial{w_{1}}} \\\\\n",
    "           \\frac{\\partial{J}}{\\partial{w_{2}}} \\\\\n",
    "           \\frac{\\partial{J}}{\\partial{w_{3}}} \\\\\n",
    "           \\frac{\\partial{J}}{\\partial{b}} \n",
    "         \\end{bmatrix}$, to perform gradient descent :\n",
    "         \n",
    "        - Repeat until convergence {\n",
    "\n",
    "            - $w_{1} = w_{1} - \\alpha.\\frac{\\partial{J}}{\\partial{w_{1}}}$\n",
    "            - $w_{2} = w_{2} - \\alpha.\\frac{\\partial{J}}{\\partial{w_{2}}}$\n",
    "            - $w_{3} = w_{3} - \\alpha.\\frac{\\partial{J}}{\\partial{w_{3}}}$\n",
    "            - $b = b - \\alpha.\\frac{\\partial{J}}{\\partial{b}}$\n",
    "\n",
    "            }\n",
    "    \n",
    "\n",
    "<h1> Generalization</h1>\n",
    "\n",
    "\n",
    "Let's take an example just for the purpose of illustation :\n",
    " \n",
    "<img src='https://miro.medium.com/max/2636/1*Gh5PS4R_A5drl5ebd_gNrg@2x.png' width='450'>\n",
    "    \n",
    "    \n",
    "<h4>1) Forward propagation :</h4>\n",
    "\n",
    "Input features $X = \\begin{bmatrix} \n",
    "    x_{1}^{(1)} & x_{1}^{(2)} & \\dots & x_{1}^{(m)} \\\\\n",
    "    x_{2}^{(1)} & x_{2}^{(2)} & \\dots & x_{2}^{(m)} \\\\\n",
    "    \\vdots & \\ddots & \\\\\n",
    "    x_{n}^{(1)} & &       & x_{n}^{(m)} \n",
    "    \\end{bmatrix}$, m training examples, every example $x^{(i)}\\in \\mathbb{R}^{n}$ so $X\\in \\mathbb{R}^{nm}$.\n",
    "    \n",
    "- For every layer $l$ we associate to it $w_{l} \\in \\mathbb{R}^{N_{l}xN_{l+1}}$ and $b_{l}\\in \\mathbb{R}^{N_{l}}$ where $N_{l}$ is the number of units in layer l. \n",
    "\n",
    "- For the example in the image: \n",
    "    - $w_{0}\\in \\mathbb{R}^{3x4}$\n",
    "    - $b_{0}\\in \\mathbb{R}^{1x3}$\n",
    "    - $X = \\begin{bmatrix} \n",
    "    x_{1}^{(1)} & x_{1}^{(2)} & \\dots & x_{1}^{(m)} \\\\\n",
    "    x_{2}^{(1)} & x_{2}^{(2)} & \\dots & x_{2}^{(m)} \\\\\n",
    "    x_{3}^{(1)} & x_{3}^{(2)} & \\dots & x_{3}^{(m)} \n",
    "    \\end{bmatrix}$\n",
    "    \n",
    "    - The first hidden layer input is : $g^{1}(Z^{1}) = g^{1}(w_{0}^{T}.X + b_{0})$, where $g^{1}$ is the activation function for layer 1. Examples of activation functions : $sigmoid$, $tanh$, $Relu$, $Leaky$ $Relu$...\n",
    "    - Generally, $sigmoid$ is used for the last output.\n",
    "    \n",
    "Now let's generlize our equations :\n",
    "\n",
    "- For every layer the input features is calculated like the following:\n",
    "    - 1) $Z^{l} = w_{l}^{T}.A^{l-1} + b_{l}$. $Z^{l} \\in \\mathbb{R}^{N_{l}.m}$.\n",
    "    - 2) $A^{l} =g^{l}(Z^{l}) $. I used capital letter because there are m training examples.$A^{l} \\in \\mathbb{R}^{N_{l}.m}$\n",
    "    \n",
    "- The output, layer $L$ : \n",
    "    - $Z^{L} = w_{L}^{T}.A^{L-1} + b_{L}$\n",
    "    - $A^{L} =sigmoid(Z^{L}) $\n",
    "    - If we have binary classification, then $A^{L} \\in \\mathbb{R}^{1xm}$ and so $b_{L}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
