{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Explaining Backpropagation easily</h1>\n",
    "\n",
    "To better understand backpropagation let's take the following example :\n",
    "\n",
    "<img src='https://miro.medium.com/max/1086/1*dkpb3XSLslX9IjIAGrSYsA.png' width='300'>\n",
    "\n",
    "and let y = $sigmoid(w_{1}.x_{1}+w_{2}.x_{2}+w_{3}.x_{3}+b)$. \n",
    "\n",
    "- This simple form of the neural network is simply a logistic regression classifier. \n",
    "\n",
    "- Let $L(y^{k},y_{true}^{k})$ the loss function for the k$^{th}$ training example and $J(w,b) = \\frac{1}{m}\\sum_{i=1}^{m} L(y^{i},y_{true}^{i})$ the cost function defined as the sum of all the losses corresponding to m training examples.\n",
    "\n",
    "\n",
    "\n",
    "- Now our goal is to get w and b that minimize the cost function $J$. And we will do this in two main steps :\n",
    "\n",
    "   - 1) Forward propagation is how neural networks make prediction :\n",
    "       - calculate $z = w_{1}.x_{1}+w_{2}.x_{2}+w_{3}.x_{3}+b$.\n",
    "       - calculate y = $sigmoid(z)$\n",
    "       - calculate $L(y,y_{true})$ or $J(w,b)$ for many examples. \n",
    "    \n",
    "   - 2) Backward propagation is an algorithm for supervised learning of artificial neural networks using gradient descent. Given an artificial neural network and an error function, the method calculates the gradient of the error function with respect to the neural network's weights $w's$ and biases $b's$. And we will use this gradient $\\nabla(J) = \\begin{bmatrix}\n",
    "           \\frac{\\partial{J}}{\\partial{w_{1}}} \\\\\n",
    "           \\frac{\\partial{J}}{\\partial{w_{2}}} \\\\\n",
    "           \\frac{\\partial{J}}{\\partial{w_{3}}} \\\\\n",
    "           \\frac{\\partial{J}}{\\partial{b}} \n",
    "         \\end{bmatrix}$, to perform gradient descent :\n",
    "         \n",
    "        - Repeat until convergence {\n",
    "\n",
    "            - $w_{1} = w_{1} - \\alpha.\\frac{\\partial{J}}{\\partial{w_{1}}}$\n",
    "            - $w_{2} = w_{2} - \\alpha.\\frac{\\partial{J}}{\\partial{w_{2}}}$\n",
    "            - $w_{3} = w_{3} - \\alpha.\\frac{\\partial{J}}{\\partial{w_{3}}}$\n",
    "            - $b = b - \\alpha.\\frac{\\partial{J}}{\\partial{b}}$\n",
    "\n",
    "            }\n",
    "    \n",
    "\n",
    "<h1> Generalization</h1>\n",
    "\n",
    "\n",
    "Let's take an example just for the purpose of illustation :\n",
    " \n",
    "<img src='https://miro.medium.com/max/2636/1*Gh5PS4R_A5drl5ebd_gNrg@2x.png' width='450'>\n",
    "    \n",
    "    \n",
    "<h4>1) Forward propagation :</h4>\n",
    "\n",
    "Input features $X = \\begin{bmatrix} \n",
    "    x_{1}^{(1)} & x_{1}^{(2)} & \\dots & x_{1}^{(m)} \\\\\n",
    "    x_{2}^{(1)} & x_{2}^{(2)} & \\dots & x_{2}^{(m)} \\\\\n",
    "    \\vdots & \\ddots & \\\\\n",
    "    x_{n}^{(1)} & &       & x_{n}^{(m)} \n",
    "    \\end{bmatrix}$, $m$ training examples, every example $x^{(i)}\\in \\mathbb{R}^{n}$ so $X\\in \\mathbb{R}^{nm}$.\n",
    "    \n",
    "- For every layer $l$ we associate to it $w_{l} \\in \\mathbb{R}^{N_{l-1}xN_{l}}$ and $b_{l}\\in \\mathbb{R}^{N_{l}}$ where $N_{l}$ is the number of units in layer l. \n",
    "\n",
    "- For the example in the image: We will just take this convention to explain :\n",
    "    - $w_{0}\\in \\mathbb{R}^{3x4}$\n",
    "    - $b_{0}\\in \\mathbb{R}^{1x3}$\n",
    "    - $X = \\begin{bmatrix} \n",
    "    x_{1}^{(1)} & x_{1}^{(2)} & \\dots & x_{1}^{(m)} \\\\\n",
    "    x_{2}^{(1)} & x_{2}^{(2)} & \\dots & x_{2}^{(m)} \\\\\n",
    "    x_{3}^{(1)} & x_{3}^{(2)} & \\dots & x_{3}^{(m)} \n",
    "    \\end{bmatrix}$\n",
    "    \n",
    "    - The first hidden layer input is : $g^{1}(Z^{1}) = g^{1}(w_{0}^{T}.X + b_{0})$, where $g^{1}$ is the activation function for layer 1. Examples of activation functions : $sigmoid$, $tanh$, $Relu$, $Leaky$ $Relu$...\n",
    "    - Generally, $sigmoid$ is used for the last output.\n",
    "    \n",
    "Now let's generalize our equations :\n",
    "\n",
    "- For every layer the input features is calculated like the following:\n",
    "    - 1) $Z^{l} = w_{l}^{T}.A^{l-1} + b_{l}$. $Z^{l} \\in \\mathbb{R}^{N_{l}.m}$.\n",
    "    - 2) $A^{l} =g^{l}(Z^{l}) $. I used capital letter because there are m training examples.$A^{l} \\in \\mathbb{R}^{N_{l}.m}$\n",
    "    \n",
    "- The output, layer $L$ : \n",
    "    - $Z^{L} = w_{L}^{T}.A^{L-1} + b_{L}$\n",
    "    - $\\hat{y} = A^{L} =sigmoid(Z^{L}) $.\n",
    "    - If we have a binary classification, then $A^{L} \\in \\mathbb{R}^{1xm}$ and so $b_{L}$.\n",
    "    - Loss : if we are using the cross-entropy loss then  $L = -\\frac{1}{m} \\sum \\limits_{i=1}^m y_{i}log({\\hat{y_{i}}}) + (1-y_{i})log(1-{\\hat{y_{i}}}),$ where $y_{i}$ is the actual output for the i$^{th}$ example and  $\\hat{y_{i}}$ the predicted output.\n",
    "    \n",
    "<h4>2) Back propagation :</h4>\n",
    "\n",
    "- Here how it's going : First we calculate the output and then derive the loss $L$ w.r.t $w$ and $b$ and plug those partial derivatives in an optimizer such as gradient descent to updates $w$ and $b$.\n",
    "- So we start from the right side going back to the left side to update the weights and biases.\n",
    "\n",
    "- We will use chaine rule to get those partial  derivatives.\n",
    "\n",
    "\n",
    "***The procedure :***\n",
    "\n",
    "- Layer L :\n",
    "    - $\\frac{\\partial{L}}{\\partial{y}} = \\frac{-y}{\\hat{y}}+\\frac{1-y}{1-\\hat{y}}$\n",
    "    - $\\frac{\\partial{L}}{\\partial{Z_{L}}}$ = $\\frac{\\partial{L}}{\\partial{y}}.\\frac{\\partial{y}}{\\partial{Z_{L}}}$ :\n",
    "        - We have $y = sigmoid(Z_{L})$ so$\\frac{\\partial{y}}{\\partial{Z_{L}}} = sigmoid(Z_{L}).(1-sigmoid(Z_{L}))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
